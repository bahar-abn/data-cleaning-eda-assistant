"""
Ù…Ø§Ú˜ÙˆÙ„ ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡ (EDA)
Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø³Ø¦ÙˆÙ„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒØŒ Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ùˆ ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Union, Optional, Any
from scipy import stats
from scipy.stats import normaltest, shapiro, kstest
import warnings

from src.utils import setup_logger, detect_data_type, get_memory_usage

class EDAAnalyzer:
    """
    Ú©Ù„Ø§Ø³ ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡
    """
    
    def __init__(self, df: pd.DataFrame):
        """
        Ø³Ø§Ø²Ù†Ø¯Ù‡ Ú©Ù„Ø§Ø³ EDAAnalyzer
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
        """
        self.df = df.copy()
        self.logger = setup_logger(
            'eda_analyzer',
            log_file='outputs/logs/eda_analyzer.log'
        )
        
        self.logger.info(f"âœ… EDAAnalyzer initialized with dataframe shape: {df.shape}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ ØªØ­Ù„ÛŒÙ„
        self.analysis_results = {
            'basic_info': {},
            'descriptive_stats': {},
            'missing_values': {},
            'correlation': {},
            'distribution': {},
            'categorical_analysis': {},
            'memory_usage': {}
        }
    
    # -------------------- Ø¨Ø®Ø´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ --------------------
    
    def get_basic_info(self) -> Dict[str, Any]:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            basic_info: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡
        """
        self.logger.info("ğŸ“‹ Getting basic dataframe information...")
        
        # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ù„ÛŒ
        info = {
            'shape': {
                'rows': self.df.shape[0],
                'columns': self.df.shape[1]
            },
            'columns': self.df.columns.tolist(),
            'dtypes': {col: str(dtype) for col, dtype in self.df.dtypes.items()},
            'memory_usage': get_memory_usage(self.df),
            'index_info': {
                'start': str(self.df.index[0]) if len(self.df) > 0 else None,
                'end': str(self.df.index[-1]) if len(self.df) > 0 else None,
                'length': len(self.df.index)
            }
        }
        
        self.analysis_results['basic_info'] = info
        self.logger.info("âœ… Basic information collected")
        
        return info
    
    def get_descriptive_stats(self) -> pd.DataFrame:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            stats_df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø­Ø§ÙˆÛŒ Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ
        """
        self.logger.info("ğŸ“Š Calculating descriptive statistics...")
        
        # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) == 0:
            self.logger.warning("âš ï¸ No numeric columns found for descriptive statistics")
            return pd.DataFrame()
        
        # Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ Ù¾Ø§ÛŒÙ‡
        desc_stats = self.df[numeric_cols].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¢Ù…Ø§Ø± Ø§Ø¶Ø§ÙÛŒ
        for col in numeric_cols:
            desc_stats.loc[col, 'variance'] = self.df[col].var()
            desc_stats.loc[col, 'skewness'] = self.df[col].skew()
            desc_stats.loc[col, 'kurtosis'] = self.df[col].kurtosis()
            desc_stats.loc[col, 'missing_count'] = self.df[col].isnull().sum()
            desc_stats.loc[col, 'missing_percentage'] = (self.df[col].isnull().sum() / len(self.df)) * 100
            desc_stats.loc[col, 'unique_values'] = self.df[col].nunique()
        
        self.analysis_results['descriptive_stats'] = desc_stats.to_dict()
        self.logger.info(f"âœ… Descriptive statistics calculated for {len(numeric_cols)} numeric columns")
        
        return desc_stats
    
    # -------------------- Ø¨Ø®Ø´ ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ --------------------
    
    def calculate_correlation(self, method: str = 'pearson', threshold: float = 0.7) -> Dict[str, Any]:
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            method: Ø±ÙˆØ´ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ ('pearson', 'spearman', 'kendall')
            threshold: Ø¢Ø³ØªØ§Ù†Ù‡ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù‚ÙˆÛŒ
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            correlation_info: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
        """
        self.logger.info(f"ğŸ”„ Calculating {method} correlation matrix...")
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        numeric_cols = self.df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            self.logger.warning("âš ï¸ Need at least 2 numeric columns for correlation analysis")
            return {'matrix': None, 'high_correlations': []}
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
        corr_matrix = self.df[numeric_cols].corr(method=method)
        
        # ÛŒØ§ÙØªÙ† Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚ÙˆÛŒ
        high_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > threshold:
                    high_corr.append({
                        'variable1': corr_matrix.columns[i],
                        'variable2': corr_matrix.columns[j],
                        'correlation': round(corr_value, 3),
                        'strength': 'strong' if abs(corr_value) > 0.7 else 'moderate',
                        'direction': 'positive' if corr_value > 0 else 'negative'
                    })
        
        correlation_info = {
            'matrix': corr_matrix.to_dict(),
            'high_correlations': high_corr,
            'method': method,
            'threshold': threshold
        }
        
        self.analysis_results['correlation'] = correlation_info
        self.logger.info(f"âœ… Correlation analysis completed. Found {len(high_corr)} high correlations")
        
        return correlation_info
    
    # -------------------- Ø¨Ø®Ø´ ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ --------------------
    
    def test_normality(self, column: str) -> Dict[str, Any]:
        """
        Ø¢Ø²Ù…ÙˆÙ† Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù† ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            column: Ù†Ø§Ù… Ø³ØªÙˆÙ†
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            normality_test: Ù†ØªØ§ÛŒØ¬ Ø¢Ø²Ù…ÙˆÙ† Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù†
        """
        data = self.df[column].dropna()
        
        if len(data) < 3:
            return {'is_normal': False, 'error': 'Insufficient data'}
        
        results = {}
        
        try:
            # Shapiro-Wilk test (Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ú©Ù…ØªØ± Ø§Ø² 5000)
            if len(data) < 5000:
                shapiro_stat, shapiro_p = shapiro(data)
                results['shapiro'] = {
                    'statistic': round(shapiro_stat, 4),
                    'p_value': round(shapiro_p, 4),
                    'is_normal': shapiro_p > 0.05
                }
            
            # D'Agostino's K^2 test
            dagostino_stat, dagostino_p = normaltest(data)
            results['dagostino'] = {
                'statistic': round(dagostino_stat, 4),
                'p_value': round(dagostino_p, 4),
                'is_normal': dagostino_p > 0.05
            }
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Normality test failed for {column}: {e}")
            results['error'] = str(e)
        
        return results
    
    def analyze_distribution(self) -> Dict[str, Dict]:
        """
        ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            distribution_info: Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙˆØ²ÛŒØ¹ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ†
        """
        self.logger.info("ğŸ“ˆ Analyzing data distributions...")
        
        distribution_info = {}
        
        for column in self.df.columns:
            col_info = {}
            data_type = detect_data_type(self.df[column])
            col_info['data_type'] = data_type
            
            # Ø¢Ù…Ø§Ø± Ù¾Ø§ÛŒÙ‡
            col_info['unique_count'] = int(self.df[column].nunique())
            col_info['unique_percentage'] = round((col_info['unique_count'] / len(self.df)) * 100, 2)
            col_info['missing_count'] = int(self.df[column].isnull().sum())
            col_info['missing_percentage'] = round((col_info['missing_count'] / len(self.df)) * 100, 2)
            
            # Ø¢Ù…Ø§Ø± Ù…Ø®ØµÙˆØµ Ù‡Ø± Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡
            if data_type == 'numeric':
                col_info.update({
                    'min': float(self.df[column].min()) if not pd.isna(self.df[column].min()) else None,
                    'max': float(self.df[column].max()) if not pd.isna(self.df[column].max()) else None,
                    'mean': float(self.df[column].mean()) if not pd.isna(self.df[column].mean()) else None,
                    'median': float(self.df[column].median()) if not pd.isna(self.df[column].median()) else None,
                    'std': float(self.df[column].std()) if not pd.isna(self.df[column].std()) else None,
                    'skewness': float(self.df[column].skew()) if not pd.isna(self.df[column].skew()) else None,
                    'kurtosis': float(self.df[column].kurtosis()) if not pd.isna(self.df[column].kurtosis()) else None,
                    'q1': float(self.df[column].quantile(0.25)),
                    'q3': float(self.df[column].quantile(0.75)),
                    'iqr': float(self.df[column].quantile(0.75) - self.df[column].quantile(0.25))
                })
                
                # Ø¢Ø²Ù…ÙˆÙ† Ù†Ø±Ù…Ø§Ù„ Ø¨ÙˆØ¯Ù†
                normality_results = self.test_normality(column)
                col_info['normality_tests'] = normality_results
                
            elif data_type == 'categorical':
                # ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ±
                value_counts = self.df[column].value_counts().head(10).to_dict()
                # ØªØ¨Ø¯ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ø±Ø´ØªÙ‡
                col_info['top_values'] = {str(k): int(v) for k, v in value_counts.items()}
                col_info['mode'] = str(self.df[column].mode()[0]) if not self.df[column].mode().empty else None
                
            elif data_type == 'datetime':
                col_info.update({
                    'min_date': str(self.df[column].min()) if not pd.isna(self.df[column].min()) else None,
                    'max_date': str(self.df[column].max()) if not pd.isna(self.df[column].max()) else None,
                    'range_days': (self.df[column].max() - self.df[column].min()).days if not pd.isna(self.df[column].min()) else None
                })
            
            distribution_info[column] = col_info
        
        self.analysis_results['distribution'] = distribution_info
        self.logger.info(f"âœ… Distribution analysis completed for {len(self.df.columns)} columns")
        
        return distribution_info
    
    # -------------------- Ø¨Ø®Ø´ ØªØ­Ù„ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ --------------------
    
    def analyze_categorical(self) -> Dict[str, Dict]:
        """
        ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            categorical_info: Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªØ­Ù„ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        """
        self.logger.info("ğŸ·ï¸ Analyzing categorical columns...")
        
        categorical_info = {}
        categorical_cols = self.df.select_dtypes(include=['object', 'category']).columns
        
        for column in categorical_cols:
            col_info = {}
            
            # Ø¢Ù…Ø§Ø± Ú©Ù„ÛŒ
            col_info['total_values'] = len(self.df[column])
            col_info['unique_values'] = self.df[column].nunique()
            col_info['missing_values'] = int(self.df[column].isnull().sum())
            col_info['missing_percentage'] = round((col_info['missing_values'] / len(self.df)) * 100, 2)
            
            # Ù¾Ø±ØªÚ©Ø±Ø§Ø±ØªØ±ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ±
            top_values = self.df[column].value_counts().head(10)
            col_info['top_values'] = {str(k): int(v) for k, v in top_values.items()}
            col_info['top_value_frequency'] = float(top_values.iloc[0] / len(self.df) * 100) if len(top_values) > 0 else 0
            
            # Ú©Ù…â€ŒØªÚ©Ø±Ø§Ø±ØªØ±ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ±
            rare_threshold = len(self.df) * 0.01  # 1%
            rare_values = self.df[column].value_counts()
            rare_values = rare_values[rare_values < rare_threshold]
            col_info['rare_values_count'] = len(rare_values)
            col_info['rare_values_percentage'] = (len(rare_values) / col_info['unique_values'] * 100) if col_info['unique_values'] > 0 else 0
            
            # Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ùˆ ØªÙ†ÙˆØ¹
            probabilities = self.df[column].value_counts(normalize=True)
            entropy = -sum(p * np.log2(p) for p in probabilities)
            max_entropy = np.log2(col_info['unique_values']) if col_info['unique_values'] > 0 else 0
            col_info['entropy'] = round(entropy, 2)
            col_info['normalized_entropy'] = round(entropy / max_entropy, 2) if max_entropy > 0 else 0
            
            categorical_info[column] = col_info
        
        self.analysis_results['categorical_analysis'] = categorical_info
        self.logger.info(f"âœ… Categorical analysis completed for {len(categorical_cols)} columns")
        
        return categorical_info
    
    # -------------------- Ø¨Ø®Ø´ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ EDA --------------------
    
    def generate_full_report(self) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ EDA
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            full_report: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø­Ø§ÙˆÛŒ ØªÙ…Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        """
        self.logger.info("ğŸš€ Generating complete EDA report...")
        
        # Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        self.get_basic_info()
        self.get_descriptive_stats()
        self.calculate_correlation()
        self.analyze_distribution()
        self.analyze_categorical()
        
        # Ø®Ù„Ø§ØµÙ‡ Ú©Ù„ÛŒ
        summary = {
            'dataset_name': 'Dataset Analysis',
            'total_rows': self.df.shape[0],
            'total_columns': self.df.shape[1],
            'numeric_columns': len(self.df.select_dtypes(include=[np.number]).columns),
            'categorical_columns': len(self.df.select_dtypes(include=['object', 'category']).columns),
            'datetime_columns': len(self.df.select_dtypes(include=['datetime64']).columns),
            'total_missing_values': int(self.df.isnull().sum().sum()),
            'total_duplicates': int(self.df.duplicated().sum()),
            'memory_usage_mb': get_memory_usage(self.df)['mb']
        }
        
        self.analysis_results['summary'] = summary
        
        self.logger.info("âœ… EDA report generated successfully")
        
        return self.analysis_results
    
    def get_insights(self) -> List[str]:
        """
        Ø§Ø³ØªØ®Ø±Ø§Ø¬ insights Ø§Ø² ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            insights: Ù„ÛŒØ³ØªÛŒ Ø§Ø² insights Ú©Ø´Ù Ø´Ø¯Ù‡
        """
        insights = []
        
        # 1. Insight Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡
        insights.append(f"ğŸ“ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø´Ø§Ù…Ù„ {self.df.shape[0]:,} Ø±Ú©ÙˆØ±Ø¯ Ùˆ {self.df.shape[1]} ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³Øª.")
        
        # 2. Insight Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        missing_total = self.df.isnull().sum().sum()
        if missing_total > 0:
            missing_percent = (missing_total / (self.df.shape[0] * self.df.shape[1])) * 100
            insights.append(f"âš ï¸ {missing_total:,} Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ ({missing_percent:.1f}% Ø§Ø² Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§).")
        else:
            insights.append("âœ… Ù‡ÛŒÚ† Ù…Ù‚Ø¯Ø§Ø± Ú¯Ù…Ø´Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        
        # 3. Insight Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        duplicate_count = self.df.duplicated().sum()
        if duplicate_count > 0:
            duplicate_percent = (duplicate_count / self.df.shape[0]) * 100
            insights.append(f"ğŸ”„ {duplicate_count:,} Ø±Ú©ÙˆØ±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ ({duplicate_percent:.1f}%).")
        
        # 4. Insight Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
        if 'correlation' in self.analysis_results and self.analysis_results['correlation']:
            high_corr = self.analysis_results['correlation'].get('high_correlations', [])
            if high_corr:
                strong_corr = [c for c in high_corr if c['strength'] == 'strong']
                insights.append(f"ğŸ“Š {len(strong_corr)} Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ù‚ÙˆÛŒ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ÙØª Ø´Ø¯.")
        
        # 5. Insight Ø¯Ø±Ø¨Ø§Ø±Ù‡ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        if 'distribution' in self.analysis_results:
            normal_cols = 0
            for col, info in self.analysis_results['distribution'].items():
                if 'normality_tests' in info and info['normality_tests']:
                    tests = info['normality_tests']
                    if 'dagostino' in tests and tests['dagostino'].get('is_normal', False):
                        normal_cols += 1
            if normal_cols > 0:
                insights.append(f"ğŸ“ˆ {normal_cols} ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø§Ø±Ø§ÛŒ ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù‡Ø³ØªÙ†Ø¯.")
        
        # 6. Insight Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        if 'categorical_analysis' in self.analysis_results:
            high_cardinality = 0
            for col, info in self.analysis_results['categorical_analysis'].items():
                if info['unique_values'] > 100:
                    high_cardinality += 1
            if high_cardinality > 0:
                insights.append(f"ğŸ·ï¸ {high_cardinality} Ø³ØªÙˆÙ† Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ ØªÙ†ÙˆØ¹ Ø¨Ø§Ù„Ø§ (Ø¨ÛŒØ´ Ø§Ø² Û±Û°Û° Ù…Ù‚Ø¯Ø§Ø± ÛŒÚ©ØªØ§) ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.")
        
        return insights