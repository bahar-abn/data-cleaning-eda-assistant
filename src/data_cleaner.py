"""
Ù…Ø§Ú˜ÙˆÙ„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡
Ø§ÛŒÙ† Ù…Ø§Ú˜ÙˆÙ„ Ù…Ø³Ø¦ÙˆÙ„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ØŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ùˆ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø§Ø³Øª
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Union, Optional, Tuple
from scipy import stats
import logging

from src.utils import setup_logger, detect_data_type, load_config

class DataCleaner:
    """
    Ú©Ù„Ø§Ø³ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡
    """
    
    def __init__(self, df: pd.DataFrame, config_path: Optional[str] = None):
        """
        Ø³Ø§Ø²Ù†Ø¯Ù‡ Ú©Ù„Ø§Ø³ DataCleaner
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… ÙˆØ±ÙˆØ¯ÛŒ
            config_path: Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        """
        self.df = df.copy()
        self.original_shape = df.shape
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
        self.config = load_config(config_path) if config_path else load_config()
        self.cleaning_config = self.config.get('cleaning', {})
        
        # ØªÙ†Ø¸ÛŒÙ… logger
        self.logger = setup_logger(
            'data_cleaner',
            log_file='outputs/logs/data_cleaner.log'
        )
        
        self.logger.info(f"âœ… DataCleaner initialized with dataframe shape: {df.shape}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        self.cleaning_report = {
            'initial_shape': self.original_shape,
            'final_shape': None,
            'missing_values': {},
            'outliers': {},
            'duplicates': 0,
            'operations': []
        }
    
    # -------------------- Ø¨Ø®Ø´ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ --------------------
    
    def analyze_missing_values(self) -> Dict[str, Dict]:
        """
        ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            missing_report: Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        """
        self.logger.info("ğŸ” Analyzing missing values...")
        
        missing_report = {}
        
        for column in self.df.columns:
            missing_count = self.df[column].isnull().sum()
            missing_percentage = (missing_count / len(self.df)) * 100
            
            missing_report[column] = {
                'count': int(missing_count),
                'percentage': round(missing_percentage, 2),
                'dtype': str(self.df[column].dtype)
            }
        
        self.cleaning_report['missing_values'] = missing_report
        self.logger.info(f"âœ… Missing values analysis completed")
        
        return missing_report
    
    def handle_missing_values(self, 
                            threshold: Optional[float] = None,
                            numeric_strategy: str = 'mean',
                            categorical_strategy: str = 'mode',
                            fill_values: Optional[Dict[str, Union[int, float, str]]] = None) -> pd.DataFrame:
        """
        Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            threshold: Ø¢Ø³ØªØ§Ù†Ù‡ Ø¯Ø±ØµØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ø³ØªÙˆÙ†
            numeric_strategy: Ø±ÙˆØ´ Ù¾Ø±Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ ('mean', 'median', 'mode', 'constant')
            categorical_strategy: Ø±ÙˆØ´ Ù¾Ø±Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ('mode', 'constant')
            fill_values: Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³ØªÙˆÙ†
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
        """
        self.logger.info("ğŸ§¹ Handling missing values...")
        
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ú¯Ø± Ù…Ù‚Ø¯Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡
        if threshold is None:
            threshold = self.cleaning_config.get('missing_values', {}).get('threshold_percent', 50)
        
        # 1. Ø­Ø°Ù Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨ÛŒØ´ Ø§Ø² Ø¢Ø³ØªØ§Ù†Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¯Ø§Ø±Ù†Ø¯
        cols_to_drop = []
        for column in self.df.columns:
            missing_percentage = (self.df[column].isnull().sum() / len(self.df)) * 100
            if missing_percentage > threshold:
                cols_to_drop.append(column)
        
        if cols_to_drop:
            self.df = self.df.drop(columns=cols_to_drop)
            self.cleaning_report['operations'].append({
                'operation': 'drop_columns_high_missing',
                'columns': cols_to_drop,
                'threshold': threshold
            })
            self.logger.info(f"âœ… Dropped columns with >{threshold}% missing values: {cols_to_drop}")
        
        # 2. Ù¾Ø±Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒÙ…Ø§Ù†Ø¯Ù‡
        for column in self.df.columns:
            if self.df[column].isnull().sum() > 0:
                # Ø§Ú¯Ø± Ù…Ù‚Ø¯Ø§Ø± Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                if fill_values and column in fill_values:
                    fill_value = fill_values[column]
                    self.df[column] = self.df[column].fillna(fill_value)
                    method = 'constant'
                else:
                    # ØªØ´Ø®ÛŒØµ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆØ´ Ù…Ù†Ø§Ø³Ø¨
                    data_type = detect_data_type(self.df[column])
                    
                    if data_type == 'numeric':
                        if numeric_strategy == 'mean':
                            fill_value = self.df[column].mean()
                        elif numeric_strategy == 'median':
                            fill_value = self.df[column].median()
                        elif numeric_strategy == 'mode':
                            fill_value = self.df[column].mode()[0] if not self.df[column].mode().empty else 0
                        else:
                            fill_value = 0
                        method = numeric_strategy
                        
                    else:  # categorical, text, datetime
                        if categorical_strategy == 'mode':
                            fill_value = self.df[column].mode()[0] if not self.df[column].mode().empty else 'Unknown'
                        else:
                            fill_value = 'Unknown'
                        method = categorical_strategy
                    
                    self.df[column] = self.df[column].fillna(fill_value)
                
                self.cleaning_report['operations'].append({
                    'operation': 'fill_missing_values',
                    'column': column,
                    'method': method,
                    'fill_value': str(fill_value) if isinstance(fill_value, (int, float)) else fill_value
                })
                
                self.logger.info(f"âœ… Filled missing values in '{column}' using {method}")
        
        return self.df
    
    # -------------------- Ø¨Ø®Ø´ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª --------------------
    
    def detect_outliers_iqr(self, column: str, multiplier: float = 1.5) -> pd.Series:
        """
        ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ø¨Ø§ Ø±ÙˆØ´ IQR
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            column: Ù†Ø§Ù… Ø³ØªÙˆÙ†
            multiplier: Ø¶Ø±ÛŒØ¨ IQR
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            outlier_mask: Ù…Ø§Ø³Ú© Ø¨ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
        """
        Q1 = self.df[column].quantile(0.25)
        Q3 = self.df[column].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - multiplier * IQR
        upper_bound = Q3 + multiplier * IQR
        
        outliers = (self.df[column] < lower_bound) | (self.df[column] > upper_bound)
        
        return outliers, lower_bound, upper_bound
    
    def detect_outliers_zscore(self, column: str, threshold: float = 3) -> pd.Series:
        """
        ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ø¨Ø§ Ø±ÙˆØ´ Z-Score
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            column: Ù†Ø§Ù… Ø³ØªÙˆÙ†
            threshold: Ø¢Ø³ØªØ§Ù†Ù‡ Z-Score
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            outlier_mask: Ù…Ø§Ø³Ú© Ø¨ÙˆÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
        """
        z_scores = np.abs(stats.zscore(self.df[column].dropna()))
        outliers = pd.Series(False, index=self.df.index)
        outliers[self.df[column].dropna().index] = z_scores > threshold
        
        return outliers
    
    def analyze_outliers(self, method: str = 'iqr') -> Dict[str, Dict]:
        """
        ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ø¯Ø± ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            method: Ø±ÙˆØ´ ØªØ´Ø®ÛŒØµ ('iqr' ÛŒØ§ 'zscore')
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            outliers_report: Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
        """
        self.logger.info(f"ğŸ” Analyzing outliers using {method} method...")
        
        outliers_report = {}
        numeric_columns = self.df.select_dtypes(include=[np.number]).columns
        
        config = self.cleaning_config.get('outliers', {})
        iqr_multiplier = config.get('iqr_multiplier', 1.5)
        zscore_threshold = config.get('zscore_threshold', 3)
        
        for column in numeric_columns:
            try:
                if method == 'iqr':
                    outliers, lower, upper = self.detect_outliers_iqr(column, iqr_multiplier)
                else:
                    outliers = self.detect_outliers_zscore(column, zscore_threshold)
                    lower = upper = None
                
                outlier_count = outliers.sum()
                outlier_percentage = (outlier_count / len(self.df)) * 100
                
                outliers_report[column] = {
                    'count': int(outlier_count),
                    'percentage': round(outlier_percentage, 2),
                    'method': method,
                    'bounds': {
                        'lower': round(lower, 2) if lower is not None else None,
                        'upper': round(upper, 2) if upper is not None else None
                    }
                }
                
            except Exception as e:
                self.logger.warning(f"âš ï¸ Could not detect outliers for {column}: {e}")
                outliers_report[column] = {'error': str(e)}
        
        self.cleaning_report['outliers'] = outliers_report
        self.logger.info(f"âœ… Outliers analysis completed")
        
        return outliers_report
    
    def handle_outliers(self, 
                       method: str = 'cap', 
                       columns: Optional[List[str]] = None,
                       iqr_multiplier: Optional[float] = None) -> pd.DataFrame:
        """
        Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            method: Ø±ÙˆØ´ Ù…Ø¯ÛŒØ±ÛŒØª ('remove', 'cap', 'winsorize')
            columns: Ù„ÛŒØ³Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù‡Ø¯Ù
            iqr_multiplier: Ø¶Ø±ÛŒØ¨ IQR
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
        """
        self.logger.info(f"ğŸ§¹ Handling outliers using {method} method...")
        
        if iqr_multiplier is None:
            iqr_multiplier = self.cleaning_config.get('outliers', {}).get('iqr_multiplier', 1.5)
        
        # Ø§Ú¯Ø± Ø³ØªÙˆÙ†ÛŒ Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ØŒ Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†
        if columns is None:
            columns = self.df.select_dtypes(include=[np.number]).columns.tolist()
        
        for column in columns:
            if column not in self.df.columns:
                continue
                
            if method == 'remove':
                # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ù¾Ø±Øª
                outliers, _, _ = self.detect_outliers_iqr(column, iqr_multiplier)
                self.df = self.df[~outliers]
                
                self.cleaning_report['operations'].append({
                    'operation': 'remove_outliers',
                    'column': column,
                    'removed_count': int(outliers.sum()),
                    'method': 'iqr'
                })
                
            elif method == 'cap':
                # Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª Ø¨Ù‡ Ú©Ø±Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§ÛŒÛŒÙ†
                outliers, lower, upper = self.detect_outliers_iqr(column, iqr_multiplier)
                self.df.loc[self.df[column] < lower, column] = lower
                self.df.loc[self.df[column] > upper, column] = upper
                
                self.cleaning_report['operations'].append({
                    'operation': 'cap_outliers',
                    'column': column,
                    'lower_bound': round(lower, 2),
                    'upper_bound': round(upper, 2)
                })
            
            self.logger.info(f"âœ… Handled outliers in '{column}' using {method}")
        
        return self.df
    
    # -------------------- Ø¨Ø®Ø´ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ --------------------
    
    def handle_duplicates(self, keep: str = 'first') -> pd.DataFrame:
        """
        Ù…Ø¯ÛŒØ±ÛŒØª Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        
        Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§:
            keep: Ú©Ø¯Ø§Ù… Ø±Ú©ÙˆØ±Ø¯ Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø´ÙˆØ¯ ('first', 'last', False)
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
        """
        self.logger.info("ğŸ” Checking for duplicate rows...")
        
        duplicate_count = self.df.duplicated().sum()
        self.cleaning_report['duplicates'] = int(duplicate_count)
        
        if duplicate_count > 0:
            self.df = self.df.drop_duplicates(keep=keep)
            self.cleaning_report['operations'].append({
                'operation': 'remove_duplicates',
                'removed_count': duplicate_count,
                'keep': keep
            })
            self.logger.info(f"âœ… Removed {duplicate_count} duplicate rows")
        else:
            self.logger.info("âœ… No duplicate rows found")
        
        return self.df
    
    # -------------------- Ø¨Ø®Ø´ ØªØºÛŒÛŒØ± Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ --------------------
    
    def optimize_dtypes(self) -> pd.DataFrame:
        """
        Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø­Ø§ÙØ¸Ù‡ Ù…ØµØ±ÙÛŒ
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø§ Ø§Ù†ÙˆØ§Ø¹ Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡
        """
        self.logger.info("ğŸ”„ Optimizing data types...")
        
        before_memory = self.df.memory_usage(deep=True).sum() / (1024 * 1024)
        
        for column in self.df.columns:
            col_type = self.df[column].dtype
            
            if col_type == 'object':
                # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ù‡ categorical Ø§Ú¯Ø± ØªØ¹Ø¯Ø§Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©ØªØ§ Ú©Ù… Ø¨Ø§Ø´Ø¯
                num_unique = self.df[column].nunique()
                if num_unique / len(self.df) < 0.5:  # Ø§Ú¯Ø± Ú©Ù…ØªØ± Ø§Ø² 50% Ù…Ù‚Ø§Ø¯ÛŒØ± ÛŒÚ©ØªØ§ Ø¨Ø§Ø´Ù†Ø¯
                    self.df[column] = self.df[column].astype('category')
                    self.cleaning_report['operations'].append({
                        'operation': 'optimize_dtype',
                        'column': column,
                        'from': 'object',
                        'to': 'category'
                    })
            
            elif 'int' in str(col_type):
                # Ú©Ø§Ù‡Ø´ Ø³Ø§ÛŒØ² Ø§Ø¹Ø¯Ø§Ø¯ ØµØ­ÛŒØ­
                c_min = self.df[column].min()
                c_max = self.df[column].max()
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    self.df[column] = self.df[column].astype('int8')
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    self.df[column] = self.df[column].astype('int16')
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    self.df[column] = self.df[column].astype('int32')
                    
            elif 'float' in str(col_type):
                # Ú©Ø§Ù‡Ø´ Ø³Ø§ÛŒØ² Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ
                self.df[column] = self.df[column].astype('float32')
        
        after_memory = self.df.memory_usage(deep=True).sum() / (1024 * 1024)
        memory_reduced = ((before_memory - after_memory) / before_memory) * 100
        
        self.logger.info(f"âœ… Data types optimized: Memory usage reduced from {before_memory:.2f} MB to {after_memory:.2f} MB ({memory_reduced:.1f}% reduction)")
        
        return self.df
    
    # -------------------- Ø¨Ø®Ø´ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ --------------------
    
    def clean_all(self) -> Tuple[pd.DataFrame, Dict]:
        """
        Ø§Ø¬Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù…Ø±Ø§Ø­Ù„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø±
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            df: Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
            report: Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ
        """
        self.logger.info("ğŸš€ Starting full data cleaning pipeline...")
        
        # 1. ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        self.analyze_missing_values()
        
        # 2. Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
        self.handle_missing_values()
        
        # 3. Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        self.handle_duplicates()
        
        # 4. ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
        self.analyze_outliers()
        
        # 5. Ù…Ø¯ÛŒØ±ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾Ø±Øª
        self.handle_outliers(method='cap')
        
        # 6. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        self.optimize_dtypes()
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        self.cleaning_report['final_shape'] = self.df.shape
        
        rows_removed = self.original_shape[0] - self.df.shape[0]
        cols_removed = self.original_shape[1] - self.df.shape[1]
        
        self.logger.info(f"âœ… Data cleaning completed: {rows_removed} rows removed, {cols_removed} columns removed")
        
        return self.df, self.cleaning_report
    
    def get_cleaning_summary(self) -> str:
        """
        Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù…ØªÙ†ÛŒ
        
        Ø¨Ø§Ø²Ú¯Ø´Øª:
            summary: Ø®Ù„Ø§ØµÙ‡ Ú¯Ø²Ø§Ø±Ø´
        """
        summary = []
        summary.append("="*50)
        summary.append("ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ Ø®Ù„Ø§ØµÙ‡ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡")
        summary.append("="*50)
        summary.append(f"ğŸ“ Ø§Ø¨Ø¹Ø§Ø¯ Ø§ÙˆÙ„ÛŒÙ‡: {self.cleaning_report['initial_shape'][0]} Ø³Ø·Ø± Ùˆ {self.cleaning_report['initial_shape'][1]} Ø³ØªÙˆÙ†")
        summary.append(f"ğŸ“ Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ: {self.cleaning_report['final_shape'][0]} Ø³Ø·Ø± Ùˆ {self.cleaning_report['final_shape'][1]} Ø³ØªÙˆÙ†")
        
        if self.cleaning_report['duplicates'] > 0:
            summary.append(f"ğŸ—‘ï¸ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡: {self.cleaning_report['duplicates']}")
        
        missing_cols = [col for col, info in self.cleaning_report['missing_values'].items() if info['percentage'] > 0]
        if missing_cols:
            summary.append(f"âš ï¸ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡: {len(missing_cols)} Ø³ØªÙˆÙ†")
        
        outlier_cols = [col for col, info in self.cleaning_report['outliers'].items() 
                       if isinstance(info, dict) and info.get('count', 0) > 0]
        if outlier_cols:
            summary.append(f"ğŸ“ˆ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ù¾Ø±Øª: {len(outlier_cols)} Ø³ØªÙˆÙ†")
        
        summary.append("="*50)
        
        return "\n".join(summary)